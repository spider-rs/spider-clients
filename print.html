<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>spider-client</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">spider-client</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/spider-rs/spider-clients/tree/main/book" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><code>spider-client</code> is a client library to use with the <a href="https://spider.cloud">Spider Cloud</a> web crawler and scraper.</p>
<ul>
<li>Concurrent</li>
<li>Streaming</li>
<li>Headless Chrome</li>
<li>HTTP Proxies</li>
<li>Cron Jobs</li>
<li>Subscriptions</li>
<li>AI Scraping and Event Driven Actions</li>
<li>Blacklisting and Budgeting Depth</li>
<li>Exponential Backoff</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simple-example"><a class="header" href="#simple-example">Simple Example</a></h1>
<p>This is a simple example of what you can do with the <code>spider-client</code> library.</p>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p>To install the library, you can use <code>pip</code> for Python or <code>npm</code> (make sure to have <a href="https://nodejs.org/en">node</a> installed) for JavaScript.:</p>
<pre><code class="language-bash"># for python
pip install spider-client
</code></pre>
<pre><code class="language-bash"># for javascript
npm install @spider-cloud/spider-client
</code></pre>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>Here is an example of how you can use the library, make sure to replace <code>your_api_key</code> with your actual API key which you can get from the <a href="https://spider.cloud">spider.cloud</a> website.</p>
<pre><code class="language-python">from spider import Spider

app = Spider(api_key='your_api_key')
url = 'https://spider.cloud'
scraped_data = app.scrape_url(url)
</code></pre>
<pre><code class="language-javascript">import { Spider } from "@spider-cloud/spider-client";

const app = new Spider({ apiKey: "your-api-key" });
const url = "https://spider.cloud";
const scrapedData = await app.scrapeUrl(url);
console.log(scrapedData);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting started</a></h1>
<p>To use the python SDK you will (of course) have to install it :)</p>
<pre><code class="language-bash">pip install spider-client
</code></pre>
<p><a href="https://pypi.org/project/spider-client/">Here</a> is the link to the package on PyPi.</p>
<h2 id="setting--getting-api-key"><a class="header" href="#setting--getting-api-key">Setting &amp; Getting Api Key</a></h2>
<p>To use the SDK you will need an API key. You can get one by signing up on <a href="https://spider.cloud?ref=python-sdk-book">spider.cloud</a>.</p>
<p>Then you need to set the API key in your environment variables.</p>
<pre><code class="language-bash">export SPIDER_API_KEY=your_api_key
</code></pre>
<p>if you don't want to set the API key in your environment variables you can pass it as an argument to the <code>Spider</code> class.</p>
<pre><code class="language-python">from spider import Spider
app = Spider(api_key='your_api_key')
</code></pre>
<p>We recommend setting the API key in your environment variables.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="crawl"><a class="header" href="#crawl">Crawl</a></h1>
<p>We will assume that you have installed the Spider package and exported your API key as an environment variable. If you haven't, please refer to the <a href="python/./getting-started.html">Getting Started</a> guide.</p>
<p>Crawl a website and return the content.</p>
<pre><code class="language-python">from spider import Spider

app = Spider()
url = "https://spider.cloud"
crawled_data = app.crawl_url(url, params={"limit": 10})
print(crawled_data)
</code></pre>
<p>The <code>crawl_url</code> method returns the content of the website in markdown format as default. We set the <code>limit</code> parameter to 10 to limit the number of pages to crawl. The maximum amount of pages allowed to crawl per website. Remove the value or set it to <code>0</code> to crawl all pages.</p>
<p>Next we will see how to crawl with with different parameters.</p>
<h2 id="crawl-with-different-parameters"><a class="header" href="#crawl-with-different-parameters">Crawl with different parameters</a></h2>
<p>The <code>crawl_url</code> method has the following parameters:</p>
<ul>
<li><code>url</code> (str): The URL of the website to crawl.</li>
</ul>
<p>the following are recommended parameters and can be set in the <code>params</code> dictionary:</p>
<ul>
<li><code>limit</code> (int): The maximum amount of pages allowed to crawl per website. Remove the value or set it to <code>0</code> to crawl all pages.</li>
<li><code>request_timeout</code> (int): The maximum amount of time to wait for a response from the website.</li>
<li><code>stealth</code> (bool): Whether to use stealth mode. Default is <code>False</code> on chrome.</li>
<li>visit the <a href="https://spider.cloud/docs/api?ref=python-sdk-book">documentation</a> for more parameters.</li>
</ul>
<pre><code class="language-python">from spider import Spider

app = Spider()
url = "https://spider.cloud"
crawled_data = app.crawl_url(
    url, params={"limit": 10, "request_timeout": 10, "stealth": True}
)

print(crawled_data)
</code></pre>
<p>If you have a lot of params, setting them inside the <code>crawl_url</code> method can be cumbersome. You can set them in a seperate <code>params</code> variable that has the <code>RequestParams</code> type which is also available in the <code>spider</code> package.</p>
<pre><code class="language-python">from spider import Spider, spider_types

params: spider_types.RequestParamsDict = {
    "limit": 10,
    "request_timeout": 10,
    "stealth": True,
    "return_format": [ "raw", "markdown" ],
    # Easier to read and intellisense will help you with the available options
}

app = Spider()
url = "https://spider.cloud"
crawled_data = app.crawl_url(url, params)

print(crawled_data)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scrape"><a class="header" href="#scrape">Scrape</a></h1>
<p>We will assume that you have installed the Spider package and exported your API key as an environment variable. If you haven't, please refer to the <a href="python/./getting-started.html">Getting Started</a> guide.</p>
<p>Scrape a website and return the content.</p>
<pre><code class="language-python">from spider import Spider

app = Spider()
url = 'https://spider.cloud'
scraped_data = app.scrape_url(url)

print(scraped_data)
</code></pre>
<p>The <code>scrape_url</code> method returns the content of the website in markdown format as default. Next we will see how to scrape with with different parameters.</p>
<h2 id="scrape-with-different-parameters"><a class="header" href="#scrape-with-different-parameters">Scrape with different parameters</a></h2>
<p>The <code>scrape_url</code> method has the following parameters:</p>
<ul>
<li><code>url</code> (str): The URL of the website to scrape.</li>
</ul>
<p>the following are optional parameters and can be set in the <code>params</code> dictionary:</p>
<ul>
<li><code>request</code> ("http", "chrome", "smart") : The type of request to make. Default is "http".</li>
<li><code>return_format</code> ("raw", "markdown", "commonmark", "html2text", "text", "bytes") : The format in which to return the scraped data. Default is "markdown".</li>
<li><code>stealth</code>, <code>anti_bot</code> and a ton of other parameters that you can find in the <a href="https://spider.cloud/docs/api?ref=python-sdk-book">documentation</a>.</li>
</ul>
<pre><code class="language-python">from spider import Spider

app = Spider()
url = "https://spider.cloud"
scraped_data = app.scrape_url(url, params={"request_timeout": 10, "stealth": True})

print(scraped_data)
</code></pre>
<p>If you have a lot of params, setting them inside the <code>scrape_url</code> method can be cumbersome. You can set them in a seperate <code>params</code> variable that has the <code>RequestParams</code> type which is also available in the <code>spider</code> package.</p>
<pre><code class="language-python">from spider import Spider, spider_types

params: spider_types.RequestParamsDict = {
    "request_timeout": 10,
    "stealth": True,
    # Easier to read and intellisense will help you with the available options
}

app = Spider()
url = "https://spider.cloud"
scraped_data = app.scrape_url(url, params)

print(scraped_data)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="async-crawl"><a class="header" href="#async-crawl">Async Crawl</a></h1>
<p>We will assume that you have installed the Spider package and exported your API key as an environment variable. If you haven't, please refer to the <a href="python/./getting-started.html">Getting Started</a> guide.</p>
<p>Crawl a website asynchronously and return the content.</p>
<pre><code class="language-python">import asyncio

from spider import AsyncSpider

url = "https://spider.cloud"


async def async_crawl_url(url, params):
    async with AsyncSpider() as app:
        crawled_data = []
        async for data in app.crawl_url(url, params=params):
            crawled_data.append(data)
    return crawled_data


result = asyncio.run(async_crawl_url(url, params={"limit": 10}))
print(result)
</code></pre>
<p>We use the <code>AsyncSpider</code> class to create an asynchronous instance of the Spider class. We then use the <code>async for</code> loop to iterate over the results of the <code>crawl_url</code> method. The <code>crawl_url</code> method returns a generator that yields the crawled data. We append the data to a list and return it. Simsalabim, we have crawled a website asynchronously.</p>
<p>Next we will see how to crawl asynchronously with different parameters.</p>
<h2 id="async-crawl-with-different-parameters"><a class="header" href="#async-crawl-with-different-parameters">Async Crawl with different parameters</a></h2>
<p>The <code>crawl_url</code> method has the following parameters:</p>
<ul>
<li><code>url</code> (str): The URL of the website to crawl.</li>
</ul>
<p>the following are recommended parameters and can be set in the <code>params</code> dictionary:</p>
<ul>
<li><code>limit</code> (int): The maximum amount of pages allowed to crawl per website. Remove the value or set it to <code>0</code> to crawl all pages.</li>
<li><code>request_timeout</code> (int): The maximum amount of time to wait for a response from the website.</li>
<li><code>stealth</code> (bool): Whether to use stealth mode. Default is <code>False</code> on chrome.</li>
<li>a ton more, visit the <a href="https://spider.cloud/docs/api?ref=python-sdk-book">documentation</a> for more parameters.</li>
</ul>
<pre><code class="language-python">import asyncio

from spider import AsyncSpider

url = "https://spider.cloud"


async def async_crawl_url(url, params):
    async with AsyncSpider() as app:
        crawled_data = []
        async for data in app.crawl_url(url, params=params):
            crawled_data.append(data)
    return crawled_data


result = asyncio.run(
    async_crawl_url(
        url,
        params={
            "limit": 10,
            "request_timeout": 10,
            "stealth": True,
            "return_format": "html",
        },
    )
)
print(result)
</code></pre>
<p>If you have a lot of params, setting them inside the <code>crawl_url</code> method can be cumbersome. You can set them in a seperate <code>params</code> variable that has the <code>RequestParams</code> type which is also available in the <code>spider</code> package.</p>
<pre><code class="language-python">import asyncio

from spider import AsyncSpider, spider_types

url = "https://spider.cloud"


async def async_crawl_url(url, params):
    async with AsyncSpider() as app:
        crawled_data = []
        async for data in app.crawl_url(url, params=params):
            crawled_data.append(data)
    return crawled_data


params: spider_types.RequestParamsDict = {
    "limit": 10,
    "request_timeout": 10,
    "stealth": True,
    # Easier to read and intellisense will help you with the available options
}

result = asyncio.run(async_crawl_url(url, params=params))
print(result)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-1"><a class="header" href="#getting-started-1">Getting started</a></h1>
<p>To be able to use the javascript SDK you will (of course) have to install it. You can do so with your package manager of choice.</p>
<pre><code class="language-bash">npm install @spider-cloud/spider-client
</code></pre>
<pre><code class="language-bash">yarn add @spider-cloud/spider-client
</code></pre>
<p><a href="https://www.npmjs.com/package/@spider-cloud/spider-client">Here</a> is the link to the package on npm.</p>
<h2 id="setting--getting-api-key-1"><a class="header" href="#setting--getting-api-key-1">Setting &amp; Getting Api Key</a></h2>
<p>To use the SDK you will need an API key. You can get one by signing up on <a href="https://spider.cloud?ref=javascript-sdk-book">spider.cloud</a>.</p>
<p>Then you need to set the API key in your environment variables.</p>
<pre><code class="language-bash">export SPIDER_API_KEY=your_api_key
</code></pre>
<p>if you don't want to set the API key in your environment variables you can pass it as an argument to the <code>Spider</code> class.</p>
<pre><code class="language-javascript">import { Spider } from "@spider-cloud/spider-client";
</code></pre>
<p>We recommend setting the API key in your environment variables.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="crawl-1"><a class="header" href="#crawl-1">Crawl</a></h1>
<p>We will assume that you have installed the Spider package and exported your API key as an environment variable. If you haven't, please refer to the <a href="javascript/./getting-started.html">Getting Started</a> guide.</p>
<p>Crawl a website and return the content.</p>
<pre><code class="language-javascript">import { Spider } from "@spider-cloud/spider-client";

const app = new Spider();
const url = "https://spider.cloud";
const scrapedData = await app.crawlUrl(url, { limit: 10 });
console.log(scrapedData);
</code></pre>
<p>The <code>crawlUrl</code> method returns the content of the website in markdown format as default. We set the <code>limit</code> parameter to 10 to limit the number of pages to crawl. The maximum amount of pages allowed to crawl per website. Remove the value or set it to <code>0</code> to crawl all pages.</p>
<p>Next we will see how to crawl with with different parameters.</p>
<h2 id="crawl-with-different-parameters-1"><a class="header" href="#crawl-with-different-parameters-1">Crawl with different parameters</a></h2>
<p>The <code>crawlUrl</code> method has the following parameters:</p>
<ul>
<li><code>url</code> (str): The URL of the website to crawl.</li>
</ul>
<p>the following are recommended parameters and can be set in the <code>params</code> dictionary:</p>
<ul>
<li><code>limit</code> (int): The maximum amount of pages allowed to crawl per website. Remove the value or set it to <code>0</code> to crawl all pages.</li>
<li><code>request_timeout</code> (int): The maximum amount of time to wait for a response from the website.</li>
<li><code>stealth</code> (bool): Whether to use stealth mode. Default is <code>False</code> on chrome.</li>
<li>visit the <a href="https://spider.cloud/docs/api?ref=javascript-sdk-book">documentation</a> for more parameters.</li>
</ul>
<pre><code class="language-javascript">import { Spider } from "@spider-cloud/spider-client";

const app = new Spider();
const url = "https://spider.cloud";
const scrapedData = await app.crawlUrl(url, {
  limit: 10,
  anti_bot: true,
  return_format: "raw",
});
console.log(scrapedData);
</code></pre>
<p>If you have a lot of params, setting them inside the <code>crawlUrl</code> method can be cumbersome. You can set them in a seperate <code>params</code> variable that has the <code>SpiderParams</code> type which is also available in the <code>spider</code> package. You will have to use Typescript if you want type annotations.</p>
<pre><code class="language-ts">import { Spider } from "@spider-cloud/spider-client";
import type { SpiderParams } from "@spider-cloud/spider-client/dist/config";

const app = new Spider();
const url = "https://spider.cloud";
const params: SpiderParams = {
  return_format: ["raw", "markdown"],
  anti_bot: true,
};
const scrapedData = await app.crawlUrl(url, params);
console.log(scrapedData);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scrape-1"><a class="header" href="#scrape-1">Scrape</a></h1>
<p>We will assume that you have installed the Spider package and exported your API key as an environment variable. If you haven't, please refer to the <a href="javascript/./getting-started.html">Getting Started</a> guide.</p>
<p>Scrape a website and return the content.</p>
<pre><code class="language-javascript">import { Spider } from "@spider-cloud/spider-client";

const app = new Spider();
const url = "https://spider.cloud";
const scrapedData = await app.scrapeUrl(url);
console.log(scrapedData);
</code></pre>
<p>The <code>scrapeUrl</code> method returns the content of the website in markdown format as default. Next we will see how to scrape with with different parameters.</p>
<h2 id="scrape-with-different-parameters-1"><a class="header" href="#scrape-with-different-parameters-1">Scrape with different parameters</a></h2>
<p>The <code>scrapeUrl</code> method has the following parameters:</p>
<ul>
<li><code>url</code> (str): The URL of the website to scrape.</li>
</ul>
<p>the following are optional parameters and can be set in the <code>params</code> dictionary:</p>
<ul>
<li><code>request</code> ("http", "chrome", "smart") : The type of request to make. Default is "http".</li>
<li><code>return_format</code> ("raw", "markdown", "commonmark", "html2text", "text", "bytes") : The format in which to return the scraped data. Default is "markdown".</li>
<li><code>stealth</code>, <code>anti_bot</code> and a ton of other parameters that you can find in the <a href="https://spider.cloud/docs/api?ref=javascript-sdk-book">documentation</a>.</li>
</ul>
<pre><code class="language-javascript">import { Spider } from "@spider-cloud/spider-client";

const app = new Spider();
const url = "https://spider.cloud";
const scrapedData = await app.scrapeUrl(url, {
  return_format: "raw",
  anti_bot: true,
});
console.log(scrapedData);
</code></pre>
<p>If you have a lot of params, setting them inside the <code>scrapeUrl</code> method can be cumbersome. You can set them in a seperate <code>params</code> variable that has the <code>SpiderParams</code> type which is also available in the <code>spider</code> package. You will have to use Typescript if you want type annotations.</p>
<pre><code class="language-ts">import { Spider } from "@spider-cloud/spider-client";
import type { SpiderParams } from "@spider-cloud/spider-client/dist/config";

const app = new Spider();
const url = "https://spider.cloud";
const params: SpiderParams = {
  return_format: "raw",
  anti_bot: true,
};
const scrapedData = await app.scrapeUrl(url, params);
console.log(scrapedData);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-2"><a class="header" href="#getting-started-2">Getting Started</a></h1>
<p>The Spider Cloud Rust SDK offers a toolkit for straightforward website scraping, crawling at scale, and other utilities like extracting links and taking screenshots, enabling you to collect data formatted for compatibility with language models (LLMs). It features a user-friendly interface for seamless integration with the Spider Cloud API.</p>
<h2 id="installation-1"><a class="header" href="#installation-1">Installation</a></h2>
<p>To use the Spider Cloud Rust SDK, include the following in your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
spider-client = "0.1"
</code></pre>
<h2 id="usage-1"><a class="header" href="#usage-1">Usage</a></h2>
<ol>
<li>Get an API key from <a href="https://spider.cloud">spider.cloud</a></li>
<li>Set the API key as an environment variable named <code>SPIDER_API_KEY</code> or pass it as an argument when creating an instance of the <code>Spider</code> struct.</li>
</ol>
<p>Here's an example of how to use the SDK:</p>
<pre><pre class="playground"><code class="language-rust">use serde_json::json;
use std::env;

#[tokio::main]
async fn main() {
    // Set the API key as an environment variable
    env::set_var("SPIDER_API_KEY", "your_api_key");

    // Initialize the Spider with your API key
    let spider = Spider::new(None).expect("API key must be provided");

    let url = "https://spider.cloud";

    // Scrape a single URL
    let scraped_data = spider.scrape_url(url, None, false, "application/json").await.expect("Failed to scrape the URL");

    println!("Scraped Data: {:?}", scraped_data);

    // Crawl a website
    let crawler_params = RequestParams {
        limit: Some(1),
        proxy_enabled: Some(true),
        store_data: Some(false),
        metadata: Some(false),
        request: Some(RequestType::Http),
        ..Default::default()
    };

    let crawl_result = spider.crawl_url(url, Some(crawler_params), false, "application/json", None::&lt;fn(serde_json::Value)&gt;).await.expect("Failed to crawl the URL");

    println!("Crawl Result: {:?}", crawl_result);
}</code></pre></pre>
<h3 id="scraping-a-url"><a class="header" href="#scraping-a-url">Scraping a URL</a></h3>
<p>To scrape data from a single URL:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let url = "https://example.com";
let scraped_data = spider.scrape_url(url, None, false, "application/json").await.expect("Failed to scrape the URL");
<span class="boring">}</span></code></pre></pre>
<h3 id="crawling-a-website"><a class="header" href="#crawling-a-website">Crawling a Website</a></h3>
<p>To automate crawling a website:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let url = "https://example.com";
let crawl_params = RequestParams {
    limit: Some(200),
    request: Some(RequestType::Smart),
    ..Default::default()
};
let crawl_result = spider.crawl_url(url, Some(crawl_params), false, "application/json", None::&lt;fn(serde_json::Value)&gt;).await.expect("Failed to crawl the URL");
<span class="boring">}</span></code></pre></pre>
<h4 id="crawl-streaming"><a class="header" href="#crawl-streaming">Crawl Streaming</a></h4>
<p>Stream crawl the website in chunks to scale with a callback:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn handle_json(json_obj: serde_json::Value) {
    println!("Received chunk: {:?}", json_obj);
}

let url = "https://example.com";
let crawl_params = RequestParams {
    limit: Some(200),
    store_data: Some(false),
    ..Default::default()
};

spider.crawl_url(
    url,
    Some(crawl_params),
    true,
    "application/json",
    Some(handle_json)
).await.expect("Failed to crawl the URL");
<span class="boring">}</span></code></pre></pre>
<h3 id="search"><a class="header" href="#search">Search</a></h3>
<p>Perform a search for websites to crawl or gather search results:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let query = "a sports website";
let crawl_params = RequestParams {
    request: Some(RequestType::Smart),
    search_limit: Some(5),
    limit: Some(5),
    fetch_page_content: Some(true),
    ..Default::default()
};
let crawl_result = spider.search(query, Some(crawl_params), false, "application/json").await.expect("Failed to perform search");
<span class="boring">}</span></code></pre></pre>
<h3 id="retrieving-links-from-a-urls"><a class="header" href="#retrieving-links-from-a-urls">Retrieving Links from a URL(s)</a></h3>
<p>Extract all links from a specified URL:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let url = "https://example.com";
let links = spider.links(url, None, false, "application/json").await.expect("Failed to retrieve links from URL");
<span class="boring">}</span></code></pre></pre>
<h3 id="transform"><a class="header" href="#transform">Transform</a></h3>
<p>Transform HTML to markdown or text lightning fast:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let data = vec![json!({"html": "&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello world&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;"})];
let params = RequestParams {
    readability: Some(false),
    return_format: Some(ReturnFormat::Markdown),
    ..Default::default()
};
let result = spider.transform(data, Some(params), false, "application/json").await.expect("Failed to transform HTML to markdown");
println!("Transformed Data: {:?}", result);
<span class="boring">}</span></code></pre></pre>
<h3 id="taking-screenshots-of-a-urls"><a class="header" href="#taking-screenshots-of-a-urls">Taking Screenshots of a URL(s)</a></h3>
<p>Capture a screenshot of a given URL:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let url = "https://example.com";
let screenshot = spider.screenshot(url, None, false, "application/json").await.expect("Failed to take screenshot of URL");
<span class="boring">}</span></code></pre></pre>
<h3 id="extracting-contact-information"><a class="header" href="#extracting-contact-information">Extracting Contact Information</a></h3>
<p>Extract contact details from a specified URL:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let url = "https://example.com";
let contacts = spider.extract_contacts(url, None, false, "application/json").await.expect("Failed to extract contacts from URL");
println!("Extracted Contacts: {:?}", contacts);
<span class="boring">}</span></code></pre></pre>
<h3 id="labeling-data-from-a-urls"><a class="header" href="#labeling-data-from-a-urls">Labeling Data from a URL(s)</a></h3>
<p>Label the data extracted from a particular URL:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let url = "https://example.com";
let labeled_data = spider.label(url, None, false, "application/json").await.expect("Failed to label data from URL");
println!("Labeled Data: {:?}", labeled_data);
<span class="boring">}</span></code></pre></pre>
<h3 id="checking-crawl-state"><a class="header" href="#checking-crawl-state">Checking Crawl State</a></h3>
<p>You can check the crawl state of a specific URL:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let url = "https://example.com";
let state = spider.get_crawl_state(url, None, false, "application/json").await.expect("Failed to get crawl state for URL");
println!("Crawl State: {:?}", state);
<span class="boring">}</span></code></pre></pre>
<h3 id="downloading-files"><a class="header" href="#downloading-files">Downloading Files</a></h3>
<p>You can download the results of the website:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let url = "https://example.com";
let options = hashmap!{
    "page" =&gt; 0,
    "limit" =&gt; 100,
    "expiresIn" =&gt; 3600 // Optional, add if needed
};
let response = spider.create_signed_url(Some(url), Some(options)).await.expect("Failed to create signed URL");
println!("Download URL: {:?}", response);
<span class="boring">}</span></code></pre></pre>
<h3 id="checking-available-credits"><a class="header" href="#checking-available-credits">Checking Available Credits</a></h3>
<p>You can check the remaining credits on your account:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let credits = spider.get_credits().await.expect("Failed to get credits");
println!("Remaining Credits: {:?}", credits);
<span class="boring">}</span></code></pre></pre>
<h3 id="data-operations"><a class="header" href="#data-operations">Data Operations</a></h3>
<p>The Spider client can now interact with specific data tables to create, retrieve, and delete data.</p>
<h4 id="retrieve-data-from-a-table"><a class="header" href="#retrieve-data-from-a-table">Retrieve Data from a Table</a></h4>
<p>To fetch data from a specified table by applying query parameters:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let table_name = "pages";
let query_params = RequestParams {
    limit: Some(20),
    ..Default::default()
};
let response = spider.data_get(table_name, Some(query_params)).await.expect("Failed to retrieve data from table");
println!("Data from table: {:?}", response);
<span class="boring">}</span></code></pre></pre>
<h4 id="delete-data-from-a-table"><a class="header" href="#delete-data-from-a-table">Delete Data from a Table</a></h4>
<p>To delete data from a specified table based on certain conditions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let table_name = "websites";
let delete_params = RequestParams {
    domain: Some("www.example.com".to_string()),
    ..Default::default()
};
let response = spider.data_delete(table_name, Some(delete_params)).await.expect("Failed to delete data from table");
println!("Delete Response: {:?}", response);
<span class="boring">}</span></code></pre></pre>
<h2 id="streaming"><a class="header" href="#streaming">Streaming</a></h2>
<p>If you need to use streaming, set the <code>stream</code> parameter to <code>true</code> and provide a callback function:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn handle_json(json_obj: serde_json::Value) {
    println!("Received chunk: {:?}", json_obj);
}

let url = "https://example.com";
let crawler_params = RequestParams {
    limit: Some(1),
    proxy_enabled: Some(true),
    store_data: Some(false),
    metadata: Some(false),
    request: Some(RequestType::Http),
    ..Default::default()
};

spider.links(url, Some(crawler_params), true, "application/json").await.expect("Failed to retrieve links from URL");
<span class="boring">}</span></code></pre></pre>
<h2 id="content-type"><a class="header" href="#content-type">Content-Type</a></h2>
<p>The following Content-type headers are supported using the <code>content_type</code> parameter:</p>
<ul>
<li><code>application/json</code></li>
<li><code>text/csv</code></li>
<li><code>application/xml</code></li>
<li><code>application/jsonl</code></li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let url = "https://example.com";

let crawler_params = RequestParams {
    limit: Some(1),
    proxy_enabled: Some(true),
    store_data: Some(false),
    metadata: Some(false),
    request: Some(RequestType::Http),
    ..Default::default()
};

// Stream JSON lines back to the client
spider.crawl_url(url, Some(crawler_params), true, "application/jsonl", None::&lt;fn(serde_json::Value)&gt;).await.expect("Failed to crawl the URL");
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<p>The SDK handles errors returned by the Spider Cloud API and raises appropriate exceptions. If an error occurs during a request, it will be propagated to the caller with a descriptive error message. By default request use a Exponential Backoff to retry as needed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-3"><a class="header" href="#getting-started-3">Getting Started</a></h1>
<p>Spider Cloud CLI is a command-line interface to interact with the <a href="https://spider.cloud">Spider Cloud</a> web crawler. It allows you to scrape, crawl, search, and perform various other web-related tasks through simple commands.</p>
<h2 id="installation-2"><a class="header" href="#installation-2">Installation</a></h2>
<p>Install the CLI using <a href="https://brew.sh/"><code>homebrew</code></a> or <a href="https://doc.rust-lang.org/cargo/"><code>cargo</code></a> from <a href="https://crates.io">crates.io</a>:</p>
<h3 id="homebrew"><a class="header" href="#homebrew">Homebrew</a></h3>
<pre><code class="language-sh">brew tap spider-rs/spider-cloud-cli
brew install spider-cloud-cli
</code></pre>
<h3 id="cargo"><a class="header" href="#cargo">Cargo</a></h3>
<pre><code class="language-sh">cargo install spider-cloud-cli
</code></pre>
<h2 id="usage-2"><a class="header" href="#usage-2">Usage</a></h2>
<p>After installing, you can use the CLI by typing <code>spider-cloud-cli</code> followed by a command and its respective arguments.</p>
<h3 id="authentication"><a class="header" href="#authentication">Authentication</a></h3>
<p>Before using most of the commands, you need to authenticate by providing an API key:</p>
<pre><code class="language-sh">spider-cloud-cli auth --api_key YOUR_API_KEY
</code></pre>
<h3 id="commands"><a class="header" href="#commands">Commands</a></h3>
<h4 id="scrape-2"><a class="header" href="#scrape-2">Scrape</a></h4>
<p>Scrape data from a specified URL.</p>
<pre><code class="language-sh">spider-cloud-cli scrape --url http://example.com
</code></pre>
<h4 id="crawl-2"><a class="header" href="#crawl-2">Crawl</a></h4>
<p>Crawl a specified URL with an optional limit on the number of pages.</p>
<pre><code class="language-sh">spider-cloud-cli crawl --url http://example.com --limit 10
</code></pre>
<h4 id="links"><a class="header" href="#links">Links</a></h4>
<p>Fetch links from a specified URL.</p>
<pre><code class="language-sh">spider-cloud-cli links --url http://example.com
</code></pre>
<h4 id="screenshot"><a class="header" href="#screenshot">Screenshot</a></h4>
<p>Take a screenshot of a specified URL.</p>
<pre><code class="language-sh">spider-cloud-cli screenshot --url http://example.com
</code></pre>
<h4 id="search-1"><a class="header" href="#search-1">Search</a></h4>
<p>Search for a query.</p>
<pre><code class="language-sh">spider-cloud-cli search --query "example query"
</code></pre>
<h4 id="transform-1"><a class="header" href="#transform-1">Transform</a></h4>
<p>Transform specified data.</p>
<pre><code class="language-sh">spider-cloud-cli transform --data "sample data"
</code></pre>
<h4 id="extract-contacts"><a class="header" href="#extract-contacts">Extract Contacts</a></h4>
<p>Extract contact information from a specified URL.</p>
<pre><code class="language-sh">spider-cloud-cli extract_contacts --url http://example.com
</code></pre>
<h4 id="label"><a class="header" href="#label">Label</a></h4>
<p>Label data from a specified URL.</p>
<pre><code class="language-sh">spider-cloud-cli label --url http://example.com
</code></pre>
<h4 id="get-crawl-state"><a class="header" href="#get-crawl-state">Get Crawl State</a></h4>
<p>Get the crawl state of a specified URL.</p>
<pre><code class="language-sh">spider-cloud-cli get_crawl_state --url http://example.com
</code></pre>
<h4 id="query"><a class="header" href="#query">Query</a></h4>
<p>Query records of a specified domain.</p>
<pre><code class="language-sh">spider-cloud-cli query --domain example.com
</code></pre>
<h4 id="get-credits"><a class="header" href="#get-credits">Get Credits</a></h4>
<p>Fetch the account credits left.</p>
<pre><code class="language-sh">spider-cloud-cli get_credits
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
